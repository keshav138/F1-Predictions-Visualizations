{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c21fd958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial libraries, seed set, data and model directories, metrics functions\n",
    "\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GroupKFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "sns.set(style='whitegrid')\n",
    "DATA_DIR = r'C:\\Users\\ASUS\\Desktop\\F1 Predictions & Visualizations\\F1-ML-Project\\data\\features'\n",
    "MODEL_DIR = r'C:\\Users\\ASUS\\Desktop\\F1 Predictions & Visualizations\\F1-ML-Project\\data\\models'\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok = True)\n",
    "\n",
    "def mae(y_true, y_pred): return mean_absolute_error(y_true, y_pred)\n",
    "def rmse(y_true, y_pred): return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "def r2(y_true, y_pred): return r2_score(y_true, y_pred)\n",
    "\n",
    "def evaluate_regression(y_true, y_pred):\n",
    "    return {\n",
    "        'MAE': float(mae(y_true, y_pred)),\n",
    "        'RMSE': float(rmse(y_true,y_pred)),\n",
    "        'R2': float(r2(y_true, y_pred))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d86bd7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1027, 57)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\ASUS\\Desktop\\F1 Predictions & Visualizations\\F1-ML-Project\\data\\features\\season_2021_round_1_features.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d55827b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0       0\n",
       "season           0\n",
       "round            0\n",
       "session          0\n",
       "driver_name      0\n",
       "driver_number    0\n",
       "team             0\n",
       "lap_number       0\n",
       "sector1_time     0\n",
       "sector2_time     0\n",
       "sector3_time     0\n",
       "is_outlap        0\n",
       "is_inlap         0\n",
       "position         0\n",
       "speed_trap       0\n",
       "compound         0\n",
       "tyre_age         0\n",
       "stint_number     0\n",
       "air_temp         0\n",
       "track_temp       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "df.isna().sum().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac794fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'lap_time'\n",
    "drop_cols =  ['lap_time']\n",
    "\n",
    "candidate_features = [c for c in df.columns if c not in drop_cols and c not in ['race_name', 'driver_name', 'gp', 'season', 'round', 'race_date']]\n",
    "\n",
    "num_features = df[candidate_features].select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "X = df[num_features].copy()\n",
    "y = df[target].copy()\n",
    "\n",
    "groups = df['round'] ## using race/round as the group for GroupKFold\n",
    "drivers = df['driver_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ec5f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "## do this when we're using the full features csv\n",
    "# rounds = df['round'].unique()\n",
    "# train_round = np.random.choice(rounds, size = int(len(rounds)*0.7), replace = False)\n",
    "\n",
    "# train_mask = df['round'].isin(train_round)\n",
    "\n",
    "#-------------------------------------------------\n",
    "# for now use 0.7 quantilee\n",
    "\n",
    "\n",
    "lap_cutoff = df['lap_time'].quantile(0.7)\n",
    "train_mask = df['lap_time'] <= lap_cutoff\n",
    "val_mask = df['lap_time'] > lap_cutoff\n",
    "\n",
    "#--------------------------------------------------\n",
    "\n",
    "X_train, X_val  = X[train_mask], X[val_mask]\n",
    "y_train, y_val = y[train_mask], y[val_mask]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cddbd778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAE': 12.671586027671909,\n",
       " 'RMSE': 19.608533942106142,\n",
       " 'R2': -0.7170647587667422}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## BASELINE 1 - GLOBAL MEAN\n",
    "## Naive Baseline = Creating a baseline to compare, this is the simplest possible guess\n",
    "## global mean is the y_train mean, and pred_mean is an array full of the same y_training data mean, basically comparing the mean of the training data\n",
    "## and the predicted mean, the simplest prediction of the varaince between the training data and the test data, using the simplest metric \n",
    "\n",
    "global_mean = y_train.mean()\n",
    "pred_mean = np.full(len(y_val), global_mean)\n",
    "\n",
    "evaluate_regression(y_val, pred_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64f204e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAE': 12.277970995223836,\n",
       " 'RMSE': 19.449801049554967,\n",
       " 'R2': -0.6893776834242131}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Baseline 2: Feature based baseline, moved from earlier, 'what the average of everyone, to whats the average of every driver', to see how much my traget\n",
    "## variable is affected by the drivers\n",
    "\n",
    "# we're checking if knowing the drivers name, helps making my predictions better\n",
    "\n",
    "driver_mean = y_train.groupby(df.loc[train_mask,'driver_name']).mean() ## per driver average for the initial 70% of the race\n",
    "\n",
    "## map to validation\n",
    "\n",
    "pred_driver_mean = df.loc[~train_mask, 'driver_name'].map(driver_mean).fillna(global_mean).values  \n",
    "evaluate_regression(y_val, pred_driver_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65b76eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAE': 1.3392664856200909,\n",
       " 'RMSE': 2.0721457744821836,\n",
       " 'R2': 0.9808249265864898}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "nums_cols = num_features\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', Ridge(alpha=1.0, random_state=RANDOM_SEED))\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(X_train[nums_cols], y_train)\n",
    "pred_lr = pipeline.predict(X_val[nums_cols])\n",
    "evaluate_regression(y_val, pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f71ea56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "driver_name\n",
       "VET    0.700592\n",
       "MSC    0.797701\n",
       "LAT    0.829032\n",
       "ALO    1.189792\n",
       "OCO    1.319799\n",
       "RUS    1.338028\n",
       "RIC    1.417728\n",
       "GAS    1.425539\n",
       "TSU    1.485703\n",
       "RAI    1.544448\n",
       "Name: error, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = df.loc[~train_mask].copy()\n",
    "val_df['pred_lr'] = pred_lr\n",
    "\n",
    "val_df['error'] = val_df['pred_lr'] - val_df['lap_time']\n",
    "val_by_driver = val_df.groupby('driver_name')['error'].apply(lambda s : np.mean(abs(s))).sort_values()\n",
    "val_by_driver.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89719e53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "F1 ML Project venv",
   "language": "python",
   "name": "f1env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
